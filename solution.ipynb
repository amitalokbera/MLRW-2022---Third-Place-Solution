{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRW 2022 : AI Driven Biomedical Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amit\n",
      "[nltk_data]     Bera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Amit\n",
      "[nltk_data]     Bera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Amit\n",
      "[nltk_data]     Bera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing all the Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy\n",
    "import warnings\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all the CSV files and removing unnecessary columns\n",
    "train = pd.read_csv(\"labelled_train_data.csv\").iloc[:, 1:]\n",
    "train['target'] = train['ctrl']\n",
    "train = train.drop(['ctrl','pert'], axis = 1)\n",
    "unlabel = pd.read_csv(\"unlabelled_train_data.csv\").iloc[:, 1:]\n",
    "test = pd.read_csv(\"data_only_test.csv\").iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining all the text preprocessing functions\n",
    "text_col = ['characteristics_ch1', 'contact_city', 'contact_country',\n",
    "       'contact_department', 'contact_institute', 'data_processing', 'extract_protocol_ch1', 'growth_protocol_ch1',\n",
    "       'hyb_protocol', 'label_ch1', 'label_protocol_ch1',\n",
    "       'molecule_ch1', 'organism_ch1', 'scan_protocol',\n",
    "       'source_name_ch1', 'title', 'treatment_protocol_ch1', 'type','description']\n",
    "\n",
    "wordnet_lemm = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text,wordnet_lemm=wordnet_lemm,stop_words=stop_words):\n",
    "    text = re.sub(\"[^a-zA-Z0-9- ]\", \" \", str(text))\n",
    "    text = text.lower()\n",
    "    text = [wordnet_lemm.lemmatize(w) for w in text.split() if w not in stop_words and len(w)>1]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def df_text(df):\n",
    "    for i in tqdm(text_col):\n",
    "        df[i] = df[i].fillna('unknown-text')\n",
    "        temp_text = []\n",
    "        for text in df[i].values:\n",
    "            temp_text.append(text_cleaner(text))\n",
    "        df[i] = temp_text\n",
    "    return df\n",
    "\n",
    "def combo_text(df):\n",
    "    df['combo'] = df['characteristics_ch1'] + \" \"+ df['description'] + \" \" + df['source_name_ch1'] + \" \" + df['title']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 70.38it/s]\n",
      "100%|██████████| 19/19 [00:07<00:00,  2.52it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.44it/s]\n"
     ]
    }
   ],
   "source": [
    "train = combo_text(train)\n",
    "train = df_text(train)\n",
    "unlabel = combo_text(unlabel)\n",
    "unlabel = df_text(unlabel)\n",
    "test = combo_text(test)\n",
    "test = df_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used cosine similarity to get those points from the unlabeled data which are similar to the train data.\n",
    "#This will help us to retain the fact that our train and unlabelled data are having similar distribution.\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "train_vec = vec.fit_transform(train['combo'].values)\n",
    "unlabel_vec = vec.transform(unlabel['combo'].values)\n",
    "unlabel_index = []\n",
    "for index, value in tqdm(enumerate(unlabel_vec)):\n",
    "    counts = 0\n",
    "    for j in train_vec:\n",
    "        if cosine_similarity(value,j) != 0.0:\n",
    "            counts += 1\n",
    "    if counts >= 300:\n",
    "        unlabel_index.append(index)\n",
    "#Will take roughly ~2 hours to run this block of cell\n",
    "#Run the below cell to download the pickle file of unlabel data indexs which are similar to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown --id 1j8KP9zsHuujiN6R-TfDntudAPeIezxXQ\n",
    "# import pickle \n",
    "# file = open('unlabel_index.p','rb')\n",
    "# unlabel_index = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3915, 51)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe with all the similar text data\n",
    "unlabel_data = unlabel.iloc[unlabel_index]\n",
    "unlabel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((623, 1747), (623,))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now training the model on the train data and predicting the labels for the unlabel data\n",
    "col_name = ['characteristics_ch1','source_name_ch1','description','title']\n",
    "#vec_dict will save the TFIDF objects of the columns in col_name\n",
    "vec_dict = {}\n",
    "all_enc = []\n",
    "for i in col_name:\n",
    "    vec = TfidfVectorizer()\n",
    "    all_enc.append(vec.fit_transform(train[i].values))\n",
    "    vec_dict[i] = vec\n",
    "X = hstack(all_enc)\n",
    "Y = train['target']\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       319\n",
      "         1.0       1.00      1.00      1.00       304\n",
      "\n",
      "    accuracy                           1.00       623\n",
      "   macro avg       1.00      1.00      1.00       623\n",
      "weighted avg       1.00      1.00      1.00       623\n",
      "\n",
      "[[319   0]\n",
      " [  0 304]]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=1811)\n",
    "dt.fit(X, Y)\n",
    "y_pred = dt.predict(X)\n",
    "print(classification_report(Y,y_pred))\n",
    "print(confusion_matrix(Y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2414\n",
       "1.0    1501\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the unlabel data to TFIDF and predicting the labels\n",
    "all_enc = []\n",
    "for i in col_name:\n",
    "    vec = vec_dict[i]\n",
    "    all_enc.append(vec.transform(unlabel_data[i].values))\n",
    "X_unlabel = hstack(all_enc)\n",
    "unlabel_data['target'] = dt.predict(X_unlabel)\n",
    "unlabel_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2391\n",
       "1.0    1524\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We did an extensive EDA and research on the textual data for train adnd unlabel data.\n",
    "#The important question to ask before using ML is, How can we solve this without using ML?\n",
    "#After EDA, we found that the text data is having a lot of important keywords which can be used to predict the labels without using Machine Learning.\n",
    "#For example, if control or ctrl appears in the text, it implies that the datapoint is a ctrl point.\n",
    "#If case appears in the text and control / ctrl doesnt appear in the same text, it implies that the datapoint is a pert point. \n",
    "#We did some research on case-control studies in biostatistics, thus we have used the keyword 'case' for classification\n",
    "#Applying the same logic to the predicted unlabel data.\n",
    "tr = []\n",
    "for i,rows in unlabel_data.iterrows():\n",
    "    if 'control' in rows['title'] or 'control' in rows['source_name_ch1'] or 'control' in rows['characteristics_ch1']:\n",
    "        tr.append(1)\n",
    "    elif 'ctrl' in rows['title'] or 'ctrl' in rows['source_name_ch1'] or 'ctrl' in rows['characteristics_ch1']:\n",
    "        tr.append(1)\n",
    "    elif ('case' in  rows['title'] and 'control' not in rows['title']) or ('case' in  rows['source_name_ch1'] and 'control' not in rows['source_name_ch1']) or ('case' in  rows['characteristics_ch1'] and 'control' not in rows['characteristics_ch1']):\n",
    "        tr.append(0)\n",
    "    else:\n",
    "        tr.append(rows['target'])\n",
    "unlabel_data['target'] = tr\n",
    "unlabel_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 52)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 600 labelled points. Using these points we got 3.9k datapoints from unlabelled data,\n",
    "#which are most similar to our train data. Out of this 3.9k datapoints, we took 1200 datapoints for our use i.e 600 of class label 0 and 600 of class label 1.\n",
    "#So our main dataset also have a balanced distribution of labels.\n",
    "b1 = unlabel_data[unlabel_data['target'] == 0].sample(600)\n",
    "b2 = unlabel_data[unlabel_data['target'] ==1].sample(600)\n",
    "unlabel_balance = pd.concat([b1,b2])\n",
    "unlabel_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1823, 52)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data = pd.concat([train, unlabel_balance])\n",
    "main_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1823, 1749), (1823,))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF vectorizer for the main data, but using the TFIDF object of train data\n",
    "all_enc = []\n",
    "for i in col_name:\n",
    "    vec = vec_dict[i]\n",
    "    all_enc.append(vec.transform(main_data[i].values))\n",
    "X = hstack(all_enc)\n",
    "Y = main_data['target'].values\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=650)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model on the main data\n",
    "dt = DecisionTreeClassifier(random_state=650)\n",
    "dt.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#Reading the test data and preproceessing it\n",
    "test = pd.read_csv(\"data_only_test.csv\").iloc[:, 1:]\n",
    "test_ = test.copy()\n",
    "test = df_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6070, 1747)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF vectorizer for the test data\n",
    "all_enc = []\n",
    "for i in col_name:\n",
    "    vec = vec_dict[i]\n",
    "    all_enc.append(vec.transform(test[i].values))\n",
    "X_test = hstack(all_enc)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the labels for the test data and applying the same logic to the predicted labels\n",
    "sub = pd.DataFrame()\n",
    "sub['geo_accession'] = test['geo_accession']\n",
    "target = dt.predict(X_test)\n",
    "test['target'] = target\n",
    "tr = []\n",
    "for i,rows in test.iterrows():\n",
    "    if 'control' in rows['title'] or 'control' in rows['source_name_ch1'] or 'control' in rows['characteristics_ch1']:\n",
    "        tr.append(1)\n",
    "    elif 'ctrl' in rows['title'] or 'ctrl' in rows['source_name_ch1'] or 'ctrl' in rows['characteristics_ch1']:\n",
    "        tr.append(1)\n",
    "    elif ('case' in  rows['title'] and 'control' not in rows['title']) or ('case' in  rows['source_name_ch1'] and 'control' not in rows['source_name_ch1']) or ('case' in  rows['characteristics_ch1'] and 'control' not in rows['characteristics_ch1']):\n",
    "        tr.append(0)\n",
    "    else:\n",
    "        tr.append(rows['target'])\n",
    "sub['ctrl'] = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([3883, 2187], dtype=int64))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of model predicted labels \n",
    "x = np.array(target)\n",
    "unique, counts = np.unique(x, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3814\n",
       "1.0    2256\n",
       "Name: ctrl, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of model predicted labels + control-case logic\n",
    "sub['ctrl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9c76cf9d9e7bfe1d01283735e19c2ba1588cc723a1d039998c8e4aafdabcff0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
